{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/aish-anand/DataScienceExamples/blob/master/Feature%20Selection-Kaggle.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0KFpK_6n4aQl"
   },
   "source": [
    "# Feature Selection with Lending Club Dataset\n",
    "\n",
    "<!-- introduction - what is FS and why do we need to do it -->\n",
    "One of the most relevant topics in machine learning is identifying which features or attributes to use in your models. In most real world problems, data often has many features but only few are related to the target we are interested in. Feature selection is the process of selecting a subset of features which are then used in model construction.\n",
    "\n",
    "We are going to look at data from a popular lending marketplace Lending Club.  Lending club has been transforming the banking system with it's peer-to-peer lending model, where it acts as a bridge between the investor and the borrowers. In this end to end example we're going to be examining different attributes that lending club collects from borrowers and how they influence the interest rates. The dataset contains loan applications from 2007 through 2018 with current loan status and payment information.\n",
    "\n",
    "Our example dataset has 150 columns containing both categorical and continuous attributes. With feature selection, we are going to create a model with as good or better accuracy while requiring lesser data. Feature selection methods can be used to identify and remove unneeded, irrelevant and redundant attributes from data that do not contribute to the accuracy of a predictive model or may in fact decrease the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wTb-NJSg41uL"
   },
   "outputs": [],
   "source": [
    "# installing verta - skip this cell if you are running this on binder!\n",
    "try:\n",
    "  import verta\n",
    "except ModuleNotFoundError:\n",
    "  !pip install verta==0.12.0a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1tnuOzkO4aQm"
   },
   "outputs": [],
   "source": [
    "# general imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# models\n",
    "from sklearn import ensemble\n",
    "from sklearn.linear_model import Ridge, Lasso, LassoLars, LinearRegression, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# metrics\n",
    "from sklearn import metrics\n",
    "\n",
    "# misc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, VarianceThreshold, RFE\n",
    "from sklearn.feature_selection import f_regression, mutual_info_regression\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.utils import shuffle\n",
    "from multiprocessing import Pool\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QPC__92m4aQp"
   },
   "source": [
    "## Cleaning and preparing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-46v5lJl4aQq"
   },
   "source": [
    "Feature selection goes hand in hand with the initial steps of cleaning and preparing your data before the training process.\n",
    "\n",
    "<img src=\"./images/FS_in_ML.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "Checking for missing values is a good first step in preparing the data. We have chosen to remove columns where more than 65% of the values are empty. By working on the dataset slice-by-slice, we have converted the categorical features to one-hot encoding. Next, we will jump into various feature selection methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KBP9dWSg4aQq"
   },
   "outputs": [],
   "source": [
    "# reading the dataset - change to point to your folder with the dataset\n",
    "loan_df = pd.read_csv(\"../input/loan.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vRszkm1R4aQs",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# looking at missing values\n",
    "# percentage of null values in each of the columns \n",
    "missing_val = loan_df.isna().sum()/len(loan_df)*100\n",
    "print(\"Columns with more than 60% missing values - \")\n",
    "print(missing_val[missing_val > 60].sort_values(ascending=False))\n",
    "# since we cannot fill in values for these columns we're going to drop these\n",
    "loan_df = loan_df.loc[:, loan_df.isnull().mean() < .35]\n",
    "print(loan_df.shape)\n",
    "# got rid of ~50 cols with that threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y-0H8IdG4aQu",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# dropping arbitrary applicant input\n",
    "loan_df.drop(columns = ['emp_title', 'title', 'initial_list_status'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FwRoCHhp4aQw",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# looking at cols of object type - convert dates to duration, emp_length to int, encode grade and subgrade, \n",
    "loan_df.select_dtypes(include='object').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NztKA3vU4aQx",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# all dates like issue_d, earliest_cr_line, last_credit_pull_d will be converted to duration\n",
    "dttoday = datetime.now().strftime('%Y-%m-%d')\n",
    "loan_df.fillna({'earliest_cr_line':dttoday, 'issue_d':dttoday, 'last_credit_pull_d':dttoday}, inplace=True)\n",
    "loan_df[['earliest_cr_line', 'issue_d', 'last_credit_pull_d']] = loan_df[['earliest_cr_line', 'issue_d', 'last_credit_pull_d']].apply(pd.to_datetime)\n",
    "\n",
    "loan_df['earliest_cr_line'] = loan_df['earliest_cr_line'].apply(lambda x: (np.timedelta64((x - pd.Timestamp(dttoday)),'D').astype(int))/-365)\n",
    "loan_df['last_credit_pull_d'] = loan_df['last_credit_pull_d'].apply(lambda x: (np.timedelta64((x - pd.Timestamp(dttoday)),'D').astype(int))/-365)\n",
    "loan_df['issue_d'] = loan_df['issue_d'].apply(lambda x: (np.timedelta64((x - pd.Timestamp(dttoday)),'D').astype(int))/-365)\n",
    "\n",
    "loan_df[['earliest_cr_line', 'issue_d', 'last_credit_pull_d']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G8zw-AVv4aQz",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# convert zip from string to int\n",
    "loan_df['zip_code'] = loan_df['zip_code'].apply(lambda x: str(x)[:3])\n",
    "loan_df['zip_code'] = pd.to_numeric(loan_df['zip_code'], errors='coerce')\n",
    "loan_df['zip_code'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UqGqfijp4aQ0",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# converting emp_years to int\n",
    "loan_df.fillna({'emp_length':'0 years', 'term':'Unknown', 'home_ownership':'Unknown',\n",
    "                'verification_status':'Unknown', 'loan_status':'Unknown', 'purpose':'Unknown',\n",
    "                'addr_state':'Unknown', 'application_type':'Unknown', 'disbursement_method':'Unknown'}, inplace=True)\n",
    "loan_df['emp_length'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y0xw1NYH4aQ2"
   },
   "outputs": [],
   "source": [
    "# converting emp_length to float\n",
    "dict_emp_length = {'10+ years':10, '6 years':6, '4 years':4, '< 1 year':0.5, '2 years':2,\n",
    "       '9 years':9, '0 years':0, '5 years':5, '3 years':3, '7 years':7, '1 year':1,\n",
    "       '8 years':8}\n",
    "loan_df['emp_length'].replace(dict_emp_length, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TjDHFWZP4aQ5"
   },
   "outputs": [],
   "source": [
    "# dropping arbitrary applicant input, post loan attributes\n",
    "to_drop = ['pymnt_plan', 'total_pymnt',\n",
    "           'total_pymnt_inv','last_pymnt_d', 'last_pymnt_amnt', 'out_prncp',\n",
    "           'out_prncp_inv', 'total_rec_prncp', 'hardship_flag', 'collection_recovery_fee',\n",
    "           'collections_12_mths_ex_med', 'debt_settlement_flag','policy_code', 'grade', 'sub_grade']\n",
    "loan_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t8Z80M8O4aQ7"
   },
   "outputs": [],
   "source": [
    "loan_dummy_df = pd.get_dummies(loan_df.drop(columns=to_drop, axis=1))\n",
    "loan_dummy_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SzLz2hJs4aQ9"
   },
   "outputs": [],
   "source": [
    "# save new dataset for checkpointing\n",
    "# loan_dummy_df.to_csv(\"loan_dummy.csv\", index=False)\n",
    "print(loan_dummy_df.shape)\n",
    "\n",
    "# splitting into test and train after treating variables\n",
    "X_train, X_test, y_train, y_test = train_test_split(loan_df, loan_df['int_rate'], \n",
    "                                                    test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pgZ7htat4aQ_"
   },
   "outputs": [],
   "source": [
    "X_train.drop(columns=['int_rate'], axis=1, inplace=True)\n",
    "X_test.drop(columns=['int_rate'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a-RRZndG4aRC"
   },
   "outputs": [],
   "source": [
    "# number of columns in the cleaned dataset\n",
    "print(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OPQYZam84aRE"
   },
   "source": [
    "### Keeping track of models\n",
    "\n",
    "With a large number of variables, hyperparameters and different types of models, it is easy to lose track of the progress we have made. To help with this problem we are going to be using the [Verta.AI](https://www.verta.ai/)'s model management platform to keep track of models and graphs that we build thoughout the process. Setting up Verta is simple and straightforward and more information can be found [here](https://verta.readme.io/docs/getting-started)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v5Ip5m9S4aRG"
   },
   "outputs": [],
   "source": [
    "# app settings\n",
    "from verta import Client\n",
    "\n",
    "# setting up\n",
    "HOST = \"app.verta.ai\"\n",
    "EMAIL = \"your-email@gmail.com\"\n",
    "DEV_KEY= \"your-dev-key\"\n",
    "PROJECT_NAME = \"Feature-Selection\"\n",
    "\n",
    "client = Client(host=HOST,\n",
    "                email=EMAIL, \n",
    "                dev_key=DEV_KEY,\n",
    "                ignore_err_conn=True)\n",
    "proj = client.set_project(PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R83UIPSW4aRI"
   },
   "source": [
    "## Getting out hands dirty\n",
    "\n",
    "Now that our cleaned datset is in place and we have our model logging set up, we will look at 5 popular methods across these categories in detail:\n",
    "\n",
    "1. Using domain knowledge\n",
    "2. Removing highly correlated features\n",
    "3. Univariate feature selection\n",
    "4. Recursive feature elimination\n",
    "5. Embedded methods\n",
    "\n",
    "For each method, we are going to use our smaller feature set which is created to train 3 different types of models -\n",
    "1. elastic net model (baseline)\n",
    "2. random forests\n",
    "3. gradient boosting regressor\n",
    "\n",
    "We have chosen these models as they generally work for large datasets, prevent overfitting and are straightforward to understand. However of the three models, the gradient boosting regressor takes the longest to train and requires hyperparameter tuning to be done for each feature set. If compute power is a constraint, skipping this model might be in your interest. \n",
    "\n",
    "*For each feature selection method, the modeling cells have been commented out to reduce the runtime of this kernel, to run any model, feel free to uncomment those lines*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WV0FfNom4aRI"
   },
   "source": [
    "### Models + Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TtJ_emAV4aRJ"
   },
   "outputs": [],
   "source": [
    "# Calculating and logging mean absolute error and root mean squared error for each model\n",
    "def get_metrics(X_train, y_train, X_test, y_test, model, run=None):\n",
    "    \n",
    "    print(\"Getting metrics for model\")\n",
    "    # train prediction\n",
    "    train_preds = model.predict(X_train)\n",
    "    # test prediction\n",
    "    test_preds = model.predict(X_test)\n",
    "\n",
    "    # Calculate the absolute errors\n",
    "    train_errors = abs(train_preds - y_train)\n",
    "    test_errors = abs(test_preds - y_test)\n",
    "    \n",
    "    # mean absolute error (mae)\n",
    "    train_mae = round(np.mean(train_errors), 4)\n",
    "    test_mae = round(np.mean(test_errors), 4)\n",
    "    print(\"train mae: \", train_mae, \" test mae: \", test_mae)\n",
    "\n",
    "    # root mean sq error\n",
    "    train_rmse = np.sqrt(metrics.mean_squared_error(y_train, train_preds))\n",
    "    test_rmse = np.sqrt(metrics.mean_squared_error(y_test, test_preds))\n",
    "    print(\"train rmse: \", train_mae, \" test rmse: \", test_mae)\n",
    "\n",
    "    \n",
    "    # log values only if run is provided    \n",
    "    if run:\n",
    "        run.log_metric(\"train_mae\", train_mae)\n",
    "        run.log_metric(\"train_rmse\", train_rmse)\n",
    "        run.log_metric(\"test_mae\", test_mae)\n",
    "        run.log_metric(\"test_rmse\", test_rmse)\n",
    "    \n",
    "    return train_mae, test_mae, train_rmse, test_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LZVOJAl04aRK"
   },
   "outputs": [],
   "source": [
    "# Elastic net model - this logs all hyperparameters, features and metrics\n",
    "def elastic_net_model(tags, feature_set, drop = True,  **params):\n",
    "    # elastic net regression\n",
    "    \n",
    "    if drop:\n",
    "        X_train_new = X_train.drop(columns = feature_set, axis = 1)\n",
    "        X_test_new = X_test.drop(columns = feature_set, axis = 1)\n",
    "    else:\n",
    "        X_train_new = X_train.drop(X_train.columns.difference(feature_set), axis = 1)\n",
    "        X_test_new = X_test.drop(X_train.columns.difference(feature_set), axis = 1)\n",
    "    \n",
    "    # grid search\n",
    "    param_grid = {'l1_ratio': [.1, .5, .7, .85, .98, 1]} \n",
    "    en_regr = GridSearchCV(ElasticNet(), param_grid, \n",
    "                           scoring='neg_mean_absolute_error',\n",
    "                           n_jobs=-1, refit=True, cv=3, verbose=20, return_train_score=True)\n",
    "    \n",
    "    en_regr.fit(X_train_new, y_train)\n",
    "    results = pd.DataFrame(en_regr.cv_results_)\n",
    "    results[['mean_test_score', 'mean_train_score']] = results[['mean_test_score', 'mean_train_score']].apply(lambda x: np.abs(x))\n",
    "\n",
    "    for _, run_result in results.iterrows():\n",
    "        run = client.set_experiment_run()\n",
    "        run.log_tags(tags)\n",
    "        \n",
    "        if drop:\n",
    "            run.log_attribute(\"removed_features\", removed_features)\n",
    "        else:\n",
    "            run.log_attribute(\"feature_set\", feature_set)\n",
    "\n",
    "        # log hyperparameters\n",
    "        run.log_hyperparameters(run_result['params'])\n",
    "        \n",
    "        # run statistics    \n",
    "        run.log_metric(\"mean_fit_time\", run_result['mean_fit_time'])\n",
    "        run.log_metric(\"std_fit_time\", run_result['std_fit_time'])\n",
    "        run.log_metric(\"mean_score_time\", run_result['mean_score_time'])\n",
    "\n",
    "        # log summary stats of validation\n",
    "        run.log_metric(\"mean_train_score\", run_result['mean_train_score'])\n",
    "        run.log_metric(\"std_train_score\", run_result['std_train_score'])  \n",
    "    \n",
    "        run.log_metric(\"mean_test_score\", run_result['mean_test_score'])\n",
    "        run.log_metric(\"std_test_score\", run_result['std_test_score']) \n",
    "  \n",
    "\n",
    "    # log best model\n",
    "    run_baseline_en = client.set_experiment_run()\n",
    "    run_baseline_en.log_hyperparameters(en_regr.best_params_)\n",
    "    run_baseline_en.log_model(key=\"elastic_net_model\", model=en_regr)\n",
    "    run_baseline_en.log_tags(tags+['best model'])\n",
    "    \n",
    "    train_mae, test_mae, train_rmse, test_rmse = get_metrics(X_train = X_train_new,\n",
    "                                                             y_train = y_train,\n",
    "                                                             X_test = X_test_new,\n",
    "                                                             y_test = y_test,\n",
    "                                                             model = en_regr,\n",
    "                                                             run = run_baseline_en)\n",
    "    return en_regr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-nrcl8NH4aRN"
   },
   "outputs": [],
   "source": [
    "# Random forest model - this logs all hyperparameters, features and metrics\n",
    "def random_forest_model(tags, feature_set, drop = True, **params):\n",
    "    # Random Forest\n",
    "    # creating new train and test sets\n",
    "    # logging features     \n",
    "    if drop:\n",
    "        X_train_new = X_train.drop(columns = feature_set, axis = 1)\n",
    "        X_test_new = X_test.drop(columns = feature_set, axis = 1)\n",
    "    else:\n",
    "        X_train_new = X_train.drop(X_train.columns.difference(feature_set), axis = 1)\n",
    "        X_test_new = X_test.drop(X_train.columns.difference(feature_set), axis = 1)\n",
    "    \n",
    "    param_grid = {\n",
    "    'n_estimators':[50],\n",
    "    'max_depth': [4, 9],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    }\n",
    "    \n",
    "    rf = GridSearchCV(RandomForestRegressor(random_state=42),\n",
    "                      param_grid, n_jobs=4, refit=True, verbose=20, cv=3, \n",
    "                      scoring='neg_mean_absolute_error',\n",
    "                      return_train_score=True)\n",
    "    \n",
    "    rf.fit(X_train_new, y_train)\n",
    "    \n",
    "    results = pd.DataFrame(rf.cv_results_)\n",
    "    results[['mean_test_score', 'mean_train_score']] = results[['mean_test_score', 'mean_train_score']].apply(lambda x: np.abs(x))\n",
    "\n",
    "    for _, run_result in results.iterrows():\n",
    "        run = client.set_experiment_run()\n",
    "        run.log_tags(tags)\n",
    "        \n",
    "        if drop:\n",
    "            run.log_attribute(\"removed_features\", feature_set)\n",
    "        else:\n",
    "            run.log_attribute(\"feature_set\", feature_set)\n",
    "\n",
    "        # log hyperparameters\n",
    "        run.log_hyperparameters(run_result['params'])\n",
    "        \n",
    "        run.log_metric(\"mean_fit_time\", run_result['mean_fit_time'])\n",
    "        run.log_metric(\"std_fit_time\", run_result['std_fit_time'])\n",
    "        run.log_metric(\"mean_score_time\", run_result['mean_score_time'])\n",
    "\n",
    "        # log summary stats of validation\n",
    "        run.log_metric(\"mean_train_score\", run_result['mean_train_score'])\n",
    "        run.log_metric(\"std_train_score\", run_result['std_train_score'])  \n",
    "    \n",
    "        run.log_metric(\"mean_test_score\", run_result['mean_test_score'])\n",
    "        run.log_metric(\"std_test_score\", run_result['std_test_score']) \n",
    "\n",
    "    # best model\n",
    "    run_rf = client.set_experiment_run()\n",
    "    run_rf.log_hyperparameters(rf.best_params_)\n",
    "    run_rf.log_model(key=\"model\", model=rf)\n",
    "    run_rf.log_tags(tags+['best model'])\n",
    "\n",
    "    # metrics for models\n",
    "    train_mae, test_mae, train_rmse, test_rmse = get_metrics(X_train = X_train_new,\n",
    "                                                             y_train = y_train,\n",
    "                                                             X_test = X_test_new,\n",
    "                                                             y_test = y_test,\n",
    "                                                             model = rf,\n",
    "                                                             run = run_rf)\n",
    "    return rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3zXUK0G94aRQ"
   },
   "outputs": [],
   "source": [
    "# GBR model - this logs all hyperparameters, features and metrics\n",
    "def gbr_model(tags, feature_set, drop = True, **params):\n",
    "    # Gradient Boosting Regressor\n",
    "    # creating new train and test sets\n",
    "    if drop:\n",
    "        X_train_new = X_train.drop(columns = feature_set, axis = 1)\n",
    "        X_test_new = X_test.drop(columns = feature_set, axis = 1)\n",
    "    else:\n",
    "        X_train_new = X_train.drop(X_train.columns.difference(feature_set), axis = 1)\n",
    "        X_test_new = X_test.drop(X_train.columns.difference(feature_set), axis = 1)\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators':[250],\n",
    "        'learning_rate': [0.1, 0.01,0.05, 0.02],\n",
    "        'max_depth': [4, 6, 9],\n",
    "        'min_samples_leaf': [3, 7, 15]\n",
    "             }\n",
    "\n",
    "    gbr = GridSearchCV(GradientBoostingRegressor(), param_grid, n_jobs=-1, refit=True, verbose=50,\n",
    "                       cv=3, scoring='neg_mean_absolute_error',\n",
    "                       return_train_score=True)\n",
    "    \n",
    "    gbr.fit(X_train_new, y_train)\n",
    "    \n",
    "    results = pd.DataFrame(gbr.cv_results_)\n",
    "    results[['mean_test_score', 'mean_train_score']] = results[['mean_test_score', 'mean_train_score']].apply(lambda x: np.abs(x))\n",
    "\n",
    "    for _, run_result in results.iterrows():\n",
    "        run = client.set_experiment_run()\n",
    "        run.log_tags(tags)\n",
    "        \n",
    "        if drop:\n",
    "            run.log_attribute(\"removed_features\", feature_set)\n",
    "        else:\n",
    "            run.log_attribute(\"feature_set\", feature_set)\n",
    "\n",
    "        # log hyperparameters\n",
    "        run.log_hyperparameters(run_result['params'])\n",
    "        \n",
    "        run.log_metric(\"mean_fit_time\", run_result['mean_fit_time'])\n",
    "        run.log_metric(\"std_fit_time\", run_result['std_fit_time'])\n",
    "        run.log_metric(\"mean_score_time\", run_result['mean_score_time'])\n",
    "\n",
    "        # log summary stats of validation\n",
    "        run.log_metric(\"mean_train_score\", run_result['mean_train_score'])\n",
    "        run.log_metric(\"std_train_score\", run_result['std_train_score'])  \n",
    "    \n",
    "        run.log_metric(\"mean_test_score\", run_result['mean_test_score'])\n",
    "        run.log_metric(\"std_test_score\", run_result['std_test_score']) \n",
    "\n",
    "\n",
    "    # log best model\n",
    "    run_gbr = client.set_experiment_run()\n",
    "    run_gbr.log_hyperparameters(gbr.best_params_)\n",
    "    run_gbr.log_model(key=\"model\", model=gbr)\n",
    "    run_gbr.log_tags(tags+['best model'])\n",
    "\n",
    "    # metrics for models\n",
    "    train_mae, test_mae, train_rmse, test_rmse = get_metrics(X_train = X_train_new,\n",
    "                                                             y_train = y_train,\n",
    "                                                             X_test = X_test_new,\n",
    "                                                             y_test = y_test,\n",
    "                                                             model = gbr,\n",
    "                                                             run = run_gbr)\n",
    "    return gbr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xj8a5iaE4aRS"
   },
   "source": [
    "### Using domain knowledge to remove features\n",
    "Getting to know the dataset and how features can be used for the problem can give one a headstart on building a good model. In our case, the dataset comes with a data dictionary which is a good place to start. We are getting rid of columns which are free text provided by the borrower and columns related to incidents after the loan was granted as these are not of use to us.\n",
    "Since we have already removed those columns in the cleaning stage, we can create a model with all the features to see how much worse the model would be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GwKiMdq34aRT"
   },
   "outputs": [],
   "source": [
    "# run_dk = client.set_experiment_run()\n",
    "loan_dummy_dk = pd.get_dummies(loan_df.drop(columns=['grade', 'sub_grade'], axis=1))\n",
    "\n",
    "\n",
    "# splitting into test and train after treating variables\n",
    "X_train_dk, X_test_dk, y_train_dk, y_test_dk = train_test_split(loan_dummy_dk, loan_dummy_dk['int_rate'], \n",
    "                                                    test_size=0.3, random_state=42)\n",
    "\n",
    "# creating new train and test sets\n",
    "X_train_dk.drop(columns=['int_rate'], axis=1, inplace=True)\n",
    "X_test_dk.drop(columns=['int_rate'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WGq4ruWn4aRU"
   },
   "outputs": [],
   "source": [
    "# # MODEL:\n",
    "# expt = client.set_experiment(\"domain_knowledge\")\n",
    "\n",
    "# rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "# rf.fit(X_train_dk, y_train)\n",
    "# # best model\n",
    "# run_rf = client.set_experiment_run()\n",
    "# run_rf.log_model(key=\"model\", model=rf)\n",
    "# run_rf.log_tags(['domain_knowledge', 'random_forest', 'no_HPP'])\n",
    "\n",
    "# # metrics for models\n",
    "# train_mae, test_mae, train_rmse, test_rmse = get_metrics(X_train = X_train_new,\n",
    "#                                                          y_train = y_train,\n",
    "#                                                          X_test = X_test_new,\n",
    "#                                                          y_test = y_test,\n",
    "#                                                          model = rf,\n",
    "#                                                          run = run_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8stp-c5M4aRW"
   },
   "source": [
    "### Removing low variance features\n",
    "\n",
    "In sklearn’s `feature_selection` module we find `VarianceThreshold`. It removes all features whose variance doesn’t meet some threshold. By default it removes features with zero variance or features that have the same value for all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ePoEaHv_4aRX"
   },
   "outputs": [],
   "source": [
    "expt_lv = client.set_experiment(\"low_variance\")\n",
    "\n",
    "# get list of all the original df columns\n",
    "all_columns = X_train.columns\n",
    "\n",
    "# instantiate VarianceThreshold object\n",
    "vt = VarianceThreshold(threshold=0.2)\n",
    "\n",
    "# fit vt to data\n",
    "vt.fit(X_train)\n",
    "\n",
    "# get the indices of the features that are being kept\n",
    "feature_indices = vt.get_support(indices=True)\n",
    "\n",
    "# remove low-variance columns from index\n",
    "feature_names = [all_columns[idx]\n",
    "                 for idx, _\n",
    "                 in enumerate(all_columns)\n",
    "                 if idx\n",
    "                 in feature_indices]\n",
    "\n",
    "# get the columns to be removed\n",
    "removed_features = list(np.setdiff1d(all_columns,\n",
    "                                     feature_names))\n",
    "print(\"Found {0} low-variance columns.\"\n",
    "      .format(len(removed_features)))\n",
    "print(\"Low-variance columns\", removed_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M7cOSAMl4aRa"
   },
   "outputs": [],
   "source": [
    "# # MODEL:\n",
    "# # elatic net regr\n",
    "# elastic_net_model(tags=[\"elastic net\", \"baseline\"], feature_set=removed_features, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oi5LG1k04aRc"
   },
   "outputs": [],
   "source": [
    "# # MODEL:\n",
    "# # random forest\n",
    "# rf_lv = random_forest_model(tags=['random forest'], feature_set=removed_features, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qQ-mKSvC4aRd"
   },
   "outputs": [],
   "source": [
    "# # MODEL:\n",
    "# # GBR\n",
    "# gbr_lv = gbr_model(tags=['gradient boosting regressor'], feature_set=removed_features, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lUDoO4q14aRe"
   },
   "source": [
    "### Removing highly correlated features\n",
    "After examining our dataframe's correlation matrix we drop highly correlated/redundant data to address multicollinearity. When these are not removed it can lead to decreased generalization performance on the test set due to high variance and less model interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eI7UeAxy4aRf"
   },
   "outputs": [],
   "source": [
    "expt = client.set_experiment(\"Correlated features\")\n",
    "\n",
    "# corr matrix\n",
    "cor = X_train.corr()\n",
    "cor.loc[:,:] = np.tril(cor, k=-1) # below main lower triangle of an array\n",
    "cor_stack = cor.stack()\n",
    "print(\"Columns with corr. greater than 0.7 - \")\n",
    "print(cor_stack[(cor_stack > 0.70) | (cor_stack < -0.70)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I-Nn2oQw4aRg"
   },
   "outputs": [],
   "source": [
    "# all cols whose corr is greater then 0.7; generated dummy columns excluded\n",
    "cols_to_drop = ['funded_amnt', 'funded_amnt_inv', 'installment', 'total_acc', 'total_rev_hi_lim','avg_cur_bal',\n",
    "                'bc_util', 'mo_sin_old_rev_tl_op', 'mo_sin_rcnt_tl', 'num_actv_rev_tl', 'num_bc_sats', \n",
    "                'num_bc_tl', 'num_il_tl', 'num_op_rev_tl','num_rev_accts','num_rev_tl_bal_gt_0',\n",
    "                'num_sats','num_tl_30dpd', 'num_tl_op_past_12m', 'percent_bc_gt_75',\n",
    "               'tax_liens', 'tot_hi_cred_lim', 'total_bc_limit', 'total_il_high_credit_limit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JqYXVko74aRk"
   },
   "outputs": [],
   "source": [
    "# looking at columns highly correlated with the target column\n",
    "int_rate_cor = X_train.apply(lambda x: x.corr(y_train))\n",
    "print(\"Columns corr. to target variable -\")\n",
    "int_rate_cor[(int_rate_cor > 0.5) | (int_rate_cor < -0.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-qm8H7SK4aRm"
   },
   "outputs": [],
   "source": [
    "# # MODEL:\n",
    "# # elatic net regr\n",
    "# elastic_net_model(tags=[\"elastic net\", \"baseline\"], feature_set=cols_to_drop, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f-hEIlrA4aRo"
   },
   "outputs": [],
   "source": [
    "# # MODEL:\n",
    "# # random forest\n",
    "# rf_corr = random_forest_model(tags=['random forest'], feature_set=cols_to_drop, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QArjYlRD4aRq"
   },
   "outputs": [],
   "source": [
    "# # MODEL:\n",
    "# # GBR\n",
    "# gbr_model(tags=['gradient boosting regressor'], feature_set=cols_to_drop, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zm-qF0sM4aRr"
   },
   "source": [
    "### Univariate feature selection\n",
    "\n",
    "Univariate feature selection works by selecting the best features based on univariate statistical tests. We can use sklearn’s `SelectKBest` to select a number of features to keep. This method uses statistical tests to select features having the highest correlation to the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mXWR8Yzf4aRs"
   },
   "outputs": [],
   "source": [
    "expt = client.set_experiment(\"f_regr\")\n",
    "\n",
    "# for regression use f_regression\n",
    "# k is the number of features selected, here it is set to 20 but can be varied\n",
    "selector_f_reg = SelectKBest(f_regression, k=20).fit(X_train, y_train)\n",
    "\n",
    "selected_cols = [d for d, s in zip(list(X_train.columns), selector_f_reg.get_support()) if s]\n",
    "print(\"K best columns: \", selected_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "azUtld674aRt"
   },
   "outputs": [],
   "source": [
    "# # MODEL:\n",
    "# # random forest\n",
    "# random_forest_model(tags=['random forest', 'k=20'], feature_set=selected_cols, drop=False)\n",
    "\n",
    "# # elatic net regr\n",
    "# elastic_net_model(tags=[\"elastic net\", \"baseline\"], feature_set=selected_cols, drop=False)\n",
    "\n",
    "# # GBR\n",
    "# gbr_model(tags=['gradient boosting regressor'], feature_set=selected_cols, drop=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JBZIVFUK4aRv"
   },
   "source": [
    "### Recursive Feature Elimination\n",
    "Recursive feature selection works by eliminating the least important features. It continues recursively until the specified number of features is reached. Recursive elimination can be used with any model that assigns weights to features, either through `coef_` or `feature_importances_`. Here we will use random forests to select the best features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xx8UNSnm4aRv",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RFE - Feature ranking with recursive feature elimination.\n",
    "expt = client.set_experiment(\"RFE\")\n",
    "\n",
    "# run\n",
    "run_rfe = client.set_experiment_run()\n",
    "\n",
    "hyperparam_rfe = {\"step\":5, \"n_features_to_select\":20}\n",
    "hyperparam_rfr = {\"n_estimators\":20, \"max_depth\":4}\n",
    "\n",
    "estimator = RandomForestRegressor(random_state = 42, n_jobs=-1, verbose=20, **hyperparam_rfr)\n",
    "selector = RFE(estimator, verbose=20, **hyperparam_rfe)\n",
    "\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "print(\"Support: \", selector.support_)\n",
    "run_rfe.log_artifact(\"support\", selector.support_)\n",
    "print(\"Ranking: \", selector.ranking_)\n",
    "run_rfe.log_artifact(\"ranking\", selector.ranking_)\n",
    "selected_cols = [d for d, s in zip(list(X_train.columns), selector.support_) if s]\n",
    "print(\"Selected Features: \", selected_cols)\n",
    "run_rfe.log_artifact(\"feature_set\", selected_cols)\n",
    "\n",
    "run_rfe.log_hyperparameters(hyperparam_rfe)\n",
    "run_rfe.log_hyperparameters(hyperparam_rfr)\n",
    "\n",
    "run_rfe.log_tags(['random forest'])\n",
    "\n",
    "train_mae, test_mae, train_rmse, test_rmse = get_metrics(X_train = X_train,\n",
    "                                                         y_train = y_train,\n",
    "                                                         X_test = X_test,\n",
    "                                                         y_test = y_test,\n",
    "                                                         model = selector,\n",
    "                                                         run = run_rfe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8dH8jPmE4aRz"
   },
   "outputs": [],
   "source": [
    "print(\"Support: \", selector.support_)\n",
    "run_rfe.log_attribute(\"support\", estimator.support_)\n",
    "print(\"Ranking: \", selector.ranking_)\n",
    "run_rfe.log_attribute(\"ranking\", selector.ranking_)\n",
    "selected_cols = [d for d, s in zip(list(X_train.columns), selector.support_) if s]\n",
    "print(\"Selected Features: \", selected_cols)\n",
    "run_rfe.log_attribute(\"feature_set\", selected_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JEeg2gjy4aR0"
   },
   "source": [
    "### Embedded methods \n",
    "\n",
    "\n",
    "Embedded methods combine the feature selection and the learning algorithm. A learning algorithm takes advantage of its own variable selection process and performs feature selection and classification simultaneously.\n",
    "\n",
    "#### Ridge\n",
    "Ridge is another way to do feature selection, where we penalize the coefficients of the model for being too large. This is done by adding a penalty or shrinkage estimator to the regression, which can be controlled with the term *lambda*. Higher the value of lambda, more the shrinkage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bWwaHrJM4aR0"
   },
   "outputs": [],
   "source": [
    "expt = client.set_experiment(\"ridge\")\n",
    "# Ridge\n",
    "hyperparam_candidates = {\n",
    "    'alpha' : np.logspace(-4, -1, 2)\n",
    "}\n",
    "hyperparam_sets = [dict(zip(hyperparam_candidates.keys(), values))\n",
    "                   for values\n",
    "                   in itertools.product(*hyperparam_candidates.values())]\n",
    "\n",
    "def run_experiment(hyperparams, **params):\n",
    "    \n",
    "    # create object to track experiment run\n",
    "    run = client.set_experiment_run()\n",
    "    run.log_tags(['ridge', 'regularization'])\n",
    "    \n",
    "    # log hyperparameters\n",
    "    run.log_hyperparameters(hyperparams)\n",
    "    print(hyperparams)\n",
    "    \n",
    "    # create and train model\n",
    "    ridge_model = Ridge(**hyperparams)\n",
    "    ridge_model.fit(X_train, y_train)\n",
    "    \n",
    "    train_mae, test_mae, train_rmse, test_rmse = get_metrics(X_train, y_train, X_test, y_test, ridge_model, run)\n",
    "    print(\"train_mae, test_mae, train_rmse, test_rmse - \", train_mae, test_mae, train_rmse, test_rmse)\n",
    "    \n",
    "with Pool() as pool:\n",
    "    pool.map(run_experiment, hyperparam_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oHfMgTEm4aR2"
   },
   "source": [
    "#### Lasso\n",
    "Lasso Regression can be used to penalize the beta coefficients in a model, and it is very similar to ridge regression. It also adds a penalty term to the cost function of a model, with a *lambda* value that must be tuned. The most important distinction from ridge regression is that Lasso regression can force the Beta coefficient to zero, which will remove that feature from the model. This is why Lasso is preferred at times, especially when you are looking to reduce model complexity. The smaller number of features a model has, the lower the complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hxto8Hgt4aR2"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "scaled_x_train = pd.DataFrame(ss.fit_transform(X_train),columns = X_train.columns)\n",
    "print(scaled_x_train.shape == X_train.shape)\n",
    "scaled_x_test = pd.DataFrame(ss.fit_transform(X_test),columns = X_test.columns)\n",
    "print(scaled_x_test.shape == X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iNWKWzT_4aR3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "expt = client.set_experiment(\"Lasso\")\n",
    "\n",
    "# Lasso\n",
    "hyperparam_candidates = {\n",
    "    'alpha' : np.logspace(-4, -1, 2)\n",
    "}\n",
    "\n",
    "hyperparam_sets = [dict(zip(hyperparam_candidates.keys(), values))\n",
    "                   for values\n",
    "                   in itertools.product(*hyperparam_candidates.values())]\n",
    "\n",
    "def run_experiment(hyperparams):\n",
    "    \n",
    "    # create object to track experiment run\n",
    "    run = client.set_experiment_run()\n",
    "    run.log_tags(['lasso', 'regularization', 'scaled'])\n",
    "    \n",
    "    # log hyperparameters\n",
    "    run.log_hyperparameters(hyperparams)\n",
    "    print(hyperparams)\n",
    "    # create and train model\n",
    "    model = Lasso(alpha=hyperparams['alpha'], max_iter = 1e5, tol=0.001)\n",
    "    model.fit(scaled_x_train, y_train)\n",
    "    \n",
    "    train_mae, test_mae, train_rmse, test_rmse = get_metrics(scaled_x_train, y_train, scaled_x_test, y_test, model, run)\n",
    "    print(\"train_mae, test_mae, train_rmse, test_rmse - \", train_mae, test_mae, train_rmse, test_rmse)\n",
    "    \n",
    "with Pool() as pool:\n",
    "    pool.map(run_experiment, hyperparam_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2_1glVIM4aR4"
   },
   "source": [
    "## Wrapping up!\n",
    "\n",
    "Overall we have seen a number of different ways to do feature selection and we have constructed about 150 different models to see how each of them perform for this dataset. Of these the best methods turned out to be:\n",
    "\n",
    "* Best model:\n",
    "  * Domain knowledge + Ridge :\n",
    "    Hyperparameters: `{alpha=0.01}`\n",
    "  * Gradient Boosting Regressor with Domain Knowledge + SelectKBest :\n",
    "    Hyperparameters: `{learning_rate=0.1, n_estimators=100, min_samples_leaf=3, max_depth=9}`\n",
    "    \n",
    "Metrics for each methods -\n",
    "\n",
    "<img src=\"./images/summary_heatmap.png\" alt=\"train_mae\" width=\"500\"/>\n",
    "\n",
    "Here is a boxplot which summarizes the spread of values for each method - \n",
    "\n",
    "<img src=\"./images/box_plot_1.png\" alt=\"train_mae\" width=\"500\"/>\n",
    "\n",
    "<img src=\"./images/box_plot_2.png\" alt=\"test_mae\" width=\"500\"/>\n",
    "\n",
    "From the above plots we see that regularization methods work well for this dataset and can be a great starting point. However if you have more compute at your disposal, combining this feature selection method with a gradient boosting would give you much higher accuracies. \n",
    "\n",
    "Here are some useful link if you would like to learn more -\n",
    "\n",
    "* [Feature Selection: A survey](http://www.realtechsupport.org/UB/ML+CT/papers/Sahin_FeatureSelectionMethods_2014.pdf)\n",
    "* [LendingHome Dataset](https://www.kaggle.com/wendykan/lending-club-loan-data) \n",
    "* Verta AI:\n",
    "  * Want to use Verta in your experiments? [Start here](https://verta.readme.io/docs/getting-started)\n",
    "  * [Why do we need model versioning?](https://medium.com/vertaai/how-to-move-fast-in-ai-without-breaking-things-3ecb74eafd18)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Feature Selection - Kaggle.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
